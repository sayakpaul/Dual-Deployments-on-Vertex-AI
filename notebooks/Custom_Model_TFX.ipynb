{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom_Model_TFX",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "environment": {
      "name": "tf2-gpu.2-4.mnightly-2021-02-02-debian-10-test",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-02-02-debian-10-test"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayakpaul/Dual-Deployments-on-Vertex-AI/blob/main/notebooks/Custom_Model_TFX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVp-9PGYFIO"
      },
      "source": [
        "In this notebook, we will build two custom models - one for Endpoint deployment and the other one for mobile deployment. We will write a TFX pipeline to run their training and export. The entire pipeline will be orchestrated using [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7gJqmqrsfqh"
      },
      "source": [
        "## References\n",
        "\n",
        "This notebook refers to the following resources and also reuses parts of the code from there: \n",
        "* [Simple TFX Pipeline for Vertex Pipelines](https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/tfx/gcp/vertex_pipelines_simple.ipynb)\n",
        "* [Vertex AI Training with TFX and Vertex Pipelines](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_vertex_training)\n",
        "* [Importing models to Vertex AI](https://cloud.google.com/vertex-ai/docs/general/import-model)\n",
        "* [Deploying a model using the Vertex AI API](https://cloud.google.com/vertex-ai/docs/predictions/deploy-model-api)\n",
        "* [MLOPs with Vertex AI](https://github.com/GoogleCloudPlatform/mlops-with-vertex-ai)\n",
        "* [Custom components TFX](https://www.tensorflow.org/tfx/tutorials/tfx/python_function_component)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D04aKMGWXjOu"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_niUhp_TY1G"
      },
      "source": [
        "# Use the latest version of pip.\n",
        "%%capture\n",
        "!pip install --upgrade pip\n",
        "!pip install --upgrade tfx==1.0.0 kfp==1.6.1\n",
        "!pip install -q --upgrade google-cloud-aiplatform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVmgQ6w1oT_Z"
      },
      "source": [
        "### ***Please restart runtime before continuing.*** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mstgsNHWoiXk"
      },
      "source": [
        "!gcloud init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl8ewjX3oXRx"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqVWpmywXngD"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wptXF0e-UXsT",
        "outputId": "d9fb988b-f433-4fc8-c69b-8a4a1c03c4cf"
      },
      "source": [
        "import tensorflow as tf\n",
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "from tfx import v1 as tfx\n",
        "print('TFX version: {}'.format(tfx.__version__))\n",
        "import kfp\n",
        "print('KFP version: {}'.format(kfp.__version__))\n",
        "\n",
        "from google.cloud import aiplatform as vertex_ai\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.5.0\n",
            "TFX version: 1.0.0\n",
            "KFP version: 1.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFYHeepnXxpZ"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPVyBrXrW-vu"
      },
      "source": [
        "GOOGLE_CLOUD_PROJECT = 'fast-ai-exploration'    #@param {type:\"string\"}\n",
        "GOOGLE_CLOUD_REGION = 'us-central1'             #@param {type:\"string\"}\n",
        "GCS_BUCKET_NAME = 'vertex-tfx'                #@param {type:\"string\"}\n",
        "\n",
        "if not (GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):\n",
        "    from absl import logging\n",
        "    logging.error('Please set all required parameters.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV-BZSvQq7YY"
      },
      "source": [
        "The location of the bucket must be a single region. Also, the bucket needs to be created in a region when [Vertex AI services are available](https://cloud.google.com/vertex-ai/docs/general/locations#available_regions). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J65KHrt4X-Fu",
        "outputId": "deaed978-2e1e-421b-fdd8-f479d1682e41"
      },
      "source": [
        "PIPELINE_NAME = 'two-way-vertex-pipelines'\n",
        "\n",
        "# Path to various pipeline artifact.\n",
        "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n",
        "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "# Paths for users' Python module.\n",
        "MODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(\n",
        "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "# Paths for input data.\n",
        "DATA_ROOT = 'gs://flowers-public/tfrecords-jpeg-224x224'\n",
        "\n",
        "# This is the path where your model will be pushed for serving.\n",
        "SERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(\n",
        "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
        "\n",
        "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PIPELINE_ROOT: gs://vertex-tfx/pipeline_root/two-way-vertex-pipelines\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQVpzyftX0y0"
      },
      "source": [
        "## Create training modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR5pU65m6nAE"
      },
      "source": [
        "_trainer_densenet_module_file = 'flower_densenet_trainer.py'\n",
        "_trainer_mobilenet_module_file = 'flower_mobilenet_trainer.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqwYrR1GYLvm",
        "outputId": "eb15ae98-a7c6-40f3-b492-5032bc28c7b8"
      },
      "source": [
        "%%writefile {_trainer_densenet_module_file}\n",
        "\n",
        "from typing import List\n",
        "from absl import logging\n",
        "from tensorflow import keras\n",
        "from tfx import v1 as tfx\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "_IMAGE_FEATURES = {\n",
        "    \"image\": tf.io.FixedLenFeature([], tf.string),  \n",
        "    \"class\": tf.io.FixedLenFeature([], tf.int64), \n",
        "    \"one_hot_class\": tf.io.VarLenFeature(tf.float32),\n",
        "}\n",
        "\n",
        "_INPUT_SHAPE = (224, 224, 3)\n",
        "_TRAIN_BATCH_SIZE = 64\n",
        "_EVAL_BATCH_SIZE = 64\n",
        "_EPOCHS = 10\n",
        "\n",
        "\n",
        "def _parse_fn(example):\n",
        "    example = tf.io.parse_single_example(example, _IMAGE_FEATURES)\n",
        "    image = tf.image.decode_jpeg(example[\"image\"], channels=3)\n",
        "    class_label = tf.cast(example[\"class\"], tf.int32)\n",
        "    return image, class_label\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[str], batch_size: int) -> tf.data.Dataset:\n",
        "    \"\"\"Generates features and label for training.\n",
        "\n",
        "    Args:\n",
        "        file_pattern: List of paths or patterns of input tfrecord files.\n",
        "        batch_size: representing the number of consecutive elements of returned\n",
        "            dataset to combine in a single batch.\n",
        "\n",
        "    Returns:\n",
        "        A dataset that contains (features, indices) tuple where features is a\n",
        "            dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Reading data from: {file_pattern}\")\n",
        "    tfrecord_filenames = tf.io.gfile.glob(file_pattern[0] + \".gz\")\n",
        "    dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "    dataset = dataset.map(_parse_fn).batch(batch_size)\n",
        "    return dataset.repeat()\n",
        "\n",
        "\n",
        "def _make_keras_model() -> tf.keras.Model:\n",
        "    \"\"\"Creates a DenseNet121-bases model for classifying flowers data.\n",
        "\n",
        "    Returns:\n",
        "    A Keras Model.\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=_INPUT_SHAPE)\n",
        "    base_model = keras.applications.DenseNet121(\n",
        "        include_top=False, input_shape=_INPUT_SHAPE, pooling=\"avg\"\n",
        "    )\n",
        "    base_model.trainable = False\n",
        "    x = keras.applications.densenet.preprocess_input(inputs)\n",
        "    x = base_model(\n",
        "        x, training=False\n",
        "    )  # Ensures BatchNorm runs in inference model in this model\n",
        "    outputs = keras.layers.Dense(5, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "\n",
        "    model.summary(print_fn=logging.info)\n",
        "    return model\n",
        "\n",
        "\n",
        "# TFX Trainer will call this function.\n",
        "def run_fn(fn_args: tfx.components.FnArgs):\n",
        "    \"\"\"Train the model based on given args.\n",
        "\n",
        "    Args:\n",
        "        fn_args: Holds args used to train the model as name/value pairs.\n",
        "    \"\"\"\n",
        "    train_dataset = _input_fn(fn_args.train_files, batch_size=_TRAIN_BATCH_SIZE)\n",
        "    eval_dataset = _input_fn(fn_args.eval_files, batch_size=_EVAL_BATCH_SIZE)\n",
        "\n",
        "    model = _make_keras_model()\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        steps_per_epoch=fn_args.train_steps,\n",
        "        validation_data=eval_dataset,\n",
        "        validation_steps=fn_args.eval_steps,\n",
        "        epochs=_EPOCHS,\n",
        "    )\n",
        "    _, acc = model.evaluate(eval_dataset, steps=fn_args.eval_steps)\n",
        "    logging.info(f\"Validation accuracy: {round(acc * 100, 2)}%\")\n",
        "    # The result of the training should be saved in `fn_args.serving_model_dir`\n",
        "    # directory.\n",
        "    model.save(fn_args.serving_model_dir, save_format=\"tf\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting flower_densenet_trainer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIFDOw5cmqd1",
        "outputId": "737d1ffc-fd98-49be-8974-8c37cf812112"
      },
      "source": [
        "%%writefile {_trainer_mobilenet_module_file}\n",
        "\n",
        "from typing import List\n",
        "from absl import logging\n",
        "from tensorflow import keras\n",
        "from tfx import v1 as tfx\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "_IMAGE_FEATURES = {\n",
        "    \"image\": tf.io.FixedLenFeature([], tf.string),  \n",
        "    \"class\": tf.io.FixedLenFeature([], tf.int64),  \n",
        "    \"one_hot_class\": tf.io.VarLenFeature(tf.float32),\n",
        "}\n",
        "\n",
        "_INPUT_SHAPE = (224, 224, 3)\n",
        "_TRAIN_BATCH_SIZE = 64\n",
        "_EVAL_BATCH_SIZE = 64\n",
        "_EPOCHS = 10\n",
        "\n",
        "\n",
        "def _parse_fn(example):\n",
        "    example = tf.io.parse_single_example(example, _IMAGE_FEATURES)\n",
        "    image = tf.image.decode_jpeg(example[\"image\"], channels=3)\n",
        "    class_label = tf.cast(example[\"class\"], tf.int32)\n",
        "    return image, class_label\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[str], batch_size: int) -> tf.data.Dataset:\n",
        "    \"\"\"Generates features and label for training.\n",
        "\n",
        "    Args:\n",
        "        file_pattern: List of paths or patterns of input tfrecord files.\n",
        "        batch_size: representing the number of consecutive elements of returned\n",
        "            dataset to combine in a single batch.\n",
        "\n",
        "    Returns:\n",
        "        A dataset that contains (features, indices) tuple where features is a\n",
        "            dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Reading data from: {file_pattern}\")\n",
        "    tfrecord_filenames = tf.io.gfile.glob(file_pattern[0] + \".gz\")\n",
        "    dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
        "    dataset = dataset.map(_parse_fn).batch(batch_size)\n",
        "    return dataset.repeat()\n",
        "\n",
        "\n",
        "def _make_keras_model() -> tf.keras.Model:\n",
        "    \"\"\"Creates a MobileNetV3-bases model for classifying flowers data.\n",
        "\n",
        "    Returns:\n",
        "    A Keras Model.\n",
        "    \"\"\"\n",
        "    inputs = keras.Input(shape=_INPUT_SHAPE)\n",
        "    base_model = keras.applications.MobileNetV3Small(\n",
        "        include_top=False, input_shape=_INPUT_SHAPE, pooling=\"avg\"\n",
        "    )\n",
        "    base_model.trainable = False\n",
        "    x = keras.applications.mobilenet_v3.preprocess_input(inputs)\n",
        "    x = base_model(\n",
        "        x, training=False\n",
        "    )  # Ensures BatchNorm runs in inference model in this model\n",
        "    outputs = keras.layers.Dense(5, activation=\"softmax\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "\n",
        "    model.summary(print_fn=logging.info)\n",
        "    return model\n",
        "\n",
        "\n",
        "# TFX Trainer will call this function.\n",
        "def run_fn(fn_args: tfx.components.FnArgs):\n",
        "    \"\"\"Train the model based on given args.\n",
        "\n",
        "    Args:\n",
        "        fn_args: Holds args used to train the model as name/value pairs.\n",
        "    \"\"\"\n",
        "    train_dataset = _input_fn(fn_args.train_files, batch_size=_TRAIN_BATCH_SIZE)\n",
        "    eval_dataset = _input_fn(fn_args.eval_files, batch_size=_EVAL_BATCH_SIZE)\n",
        "\n",
        "    model = _make_keras_model()\n",
        "    model.fit(\n",
        "        train_dataset,\n",
        "        steps_per_epoch=fn_args.train_steps,\n",
        "        validation_data=eval_dataset,\n",
        "        validation_steps=fn_args.eval_steps,\n",
        "        epochs=_EPOCHS,\n",
        "    )\n",
        "    _, acc = model.evaluate(eval_dataset, steps=fn_args.eval_steps)\n",
        "    logging.info(f\"Validation accuracy: {round(acc * 100, 2)}%\")\n",
        "    # The result of the training should be saved in `fn_args.serving_model_dir`\n",
        "    # directory.\n",
        "    model.save(fn_args.serving_model_dir, save_format=\"tf\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting flower_mobilenet_trainer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp7RTxpMoLe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ba3a61-9454-463a-9707-e7750f088ff8"
      },
      "source": [
        "!gsutil cp -r *.py {MODULE_ROOT}/\n",
        "!gsutil ls -lh {MODULE_ROOT}/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://flower_densenet_trainer.py [Content-Type=text/x-python]...\n",
            "Copying file://flower_mobilenet_trainer.py [Content-Type=text/x-python]...\n",
            "Copying file://vertex_deployer.py [Content-Type=text/x-python]...\n",
            "Copying file://vertex_uploader.py [Content-Type=text/x-python]...\n",
            "- [4 files][  8.7 KiB/  8.7 KiB]                                                \n",
            "Operation completed over 4 objects/8.7 KiB.                                      \n",
            "  3.16 KiB  2021-08-02T07:36:38Z  gs://vertex-tfx/pipeline_module/two-way-vertex-pipelines/flower_densenet_trainer.py\n",
            "  3.17 KiB  2021-08-02T07:36:38Z  gs://vertex-tfx/pipeline_module/two-way-vertex-pipelines/flower_mobilenet_trainer.py\n",
            "  1.03 KiB  2021-08-02T07:36:39Z  gs://vertex-tfx/pipeline_module/two-way-vertex-pipelines/vertex_deployer.py\n",
            "  1.31 KiB  2021-08-02T07:36:39Z  gs://vertex-tfx/pipeline_module/two-way-vertex-pipelines/vertex_uploader.py\n",
            "TOTAL: 4 objects, 8867 bytes (8.66 KiB)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qX4M-C6X4LF"
      },
      "source": [
        "## Create the pipeline\n",
        "\n",
        "To create the end-to-end pipeline, we will need to write two custom TFX components:\n",
        "\n",
        "* One will take the pushed model from `Pusher` and will upload it to Vertex AI. \n",
        "* One will deploy the uploaded model to an Endpoint.\n",
        "\n",
        "We will then need to build a Docker image using these custom components and serve the pipeline using this image. We will use [Cloud Build](https://cloud.google.com/build) in order to build the Docker image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzZdpUjypcsT"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYaypU11e-cG"
      },
      "source": [
        "_vertex_uploader_module_file = 'vertex_uploader.py'\n",
        "_vertex_deployer_module_file = 'vertex_deployer.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBj_WnLw6oRR",
        "outputId": "53d8b374-dceb-4920-c5b4-fead2383bab7"
      },
      "source": [
        "%%writefile {_vertex_uploader_module_file}\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.dsl.component.experimental.annotations import Parameter\n",
        "from tfx.types.standard_artifacts import String\n",
        "from google.cloud import aiplatform as vertex_ai\n",
        "from tfx import v1 as tfx\n",
        "from absl import logging\n",
        "\n",
        "\n",
        "@component\n",
        "def VertexUploader(\n",
        "    project: Parameter[str],\n",
        "    region: Parameter[str],\n",
        "    model_display_name: Parameter[str],\n",
        "    pushed_model_location: Parameter[str],\n",
        "    serving_image_uri: Parameter[str],\n",
        "    uploaded_model: tfx.dsl.components.OutputArtifact[String]\n",
        "):\n",
        "\n",
        "    vertex_ai.init(project=project, location=region)\n",
        "\n",
        "    pushed_model_dir = os.path.join(\n",
        "        pushed_model_location, tf.io.gfile.listdir(pushed_model_location)[-1]\n",
        "    )\n",
        "\n",
        "    logging.info(f\"Model registry location: {pushed_model_dir}\")\n",
        "\n",
        "    vertex_model = vertex_ai.Model.upload(\n",
        "        display_name=model_display_name,\n",
        "        artifact_uri=pushed_model_dir,\n",
        "        serving_container_image_uri=serving_image_uri,\n",
        "        parameters_schema_uri=None,\n",
        "        instance_schema_uri=None,\n",
        "        explanation_metadata=None,\n",
        "        explanation_parameters=None,\n",
        "    )\n",
        "\n",
        "    uploaded_model.set_string_custom_property(\"model_resource_name\", str(vertex_model.resource_name))\n",
        "    logging.info(f\"Model resource: {str(vertex_model.resource_name)}\")\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting vertex_uploader.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awNiJiZ1fZsp",
        "outputId": "a6b5b278-18ba-4121-8324-5a8381992026"
      },
      "source": [
        "%%writefile {_vertex_deployer_module_file}\n",
        "\n",
        "from tfx.dsl.component.experimental.decorators import component\n",
        "from tfx.dsl.component.experimental.annotations import Parameter\n",
        "from tfx.types.standard_artifacts import String\n",
        "from google.cloud import aiplatform as vertex_ai\n",
        "from tfx import v1 as tfx\n",
        "from absl import logging\n",
        "\n",
        "\n",
        "@component\n",
        "def VertexDeployer(\n",
        "    project: Parameter[str],\n",
        "    region: Parameter[str],\n",
        "    model_display_name: Parameter[str],\n",
        "    deployed_model_display_name: Parameter[str]\n",
        "):  \n",
        "\n",
        "    logging.info(f\"Endpoint display: {deployed_model_display_name}\")\n",
        "    vertex_ai.init(project=project, location=region)\n",
        "\n",
        "    endpoints = vertex_ai.Endpoint.list(\n",
        "        filter=f'display_name={deployed_model_display_name}', \n",
        "        order_by=\"update_time\")\n",
        "    \n",
        "    if len(endpoints) > 0:\n",
        "        logging.info(f\"Endpoint {deployed_model_display_name} already exists.\")\n",
        "        endpoint = endpoints[-1]\n",
        "    else:\n",
        "        endpoint = vertex_ai.Endpoint.create(deployed_model_display_name)\n",
        "\n",
        "    model = vertex_ai.Model.list(\n",
        "        filter=f'display_name={model_display_name}',\n",
        "        order_by=\"update_time\"\n",
        "    )[-1]\n",
        "\n",
        "    endpoint = vertex_ai.Endpoint.list(\n",
        "        filter=f'display_name={deployed_model_display_name}',\n",
        "        order_by=\"update_time\"\n",
        "    )[-1]\n",
        "\n",
        "    deployed_model = endpoint.deploy(\n",
        "        model=model,\n",
        "        # Syntax from here: https://git.io/JBQDP\n",
        "        traffic_split={\"0\": 100},\n",
        "        machine_type=\"n1-standard-4\",\n",
        "        min_replica_count=1,\n",
        "        max_replica_count=1\n",
        "    )\n",
        "\n",
        "    logging.info(f\"Model deployed to: {deployed_model}\")\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting vertex_deployer.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joRJYojpP8-H"
      },
      "source": [
        "Create a package called `custom_components` and copy the modules we just wrote. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYiXX9pOOOAX"
      },
      "source": [
        "!mkdir -p ./custom_components\n",
        "!touch ./custom_components/__init__.py\n",
        "!cp -r {_vertex_uploader_module_file} {_vertex_deployer_module_file} custom_components"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiLiGjR3OOAX",
        "outputId": "55fb4fbc-0736-470b-b0b7-dc56d70871e1"
      },
      "source": [
        "!ls -lh custom_components"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 12K\n",
            "-rw-r--r-- 1 root root    0 Aug  2 07:38 __init__.py\n",
            "drwxr-xr-x 2 root root 4.0K Aug  2 05:13 __pycache__\n",
            "-rw-r--r-- 1 root root 1.5K Aug  2 07:38 vertex_deployer.py\n",
            "-rw-r--r-- 1 root root 1.4K Aug  2 07:38 vertex_uploader.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcFpCCSbQDol"
      },
      "source": [
        "### `Dockerfile` configuration "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9j867lPfyM8",
        "outputId": "a27e938e-1b23-4bdf-9319-c7b0c98de8ae"
      },
      "source": [
        "DATASET_DISPLAY_NAME = \"flowers\"\n",
        "VERSION = \"tfx-1-0-0\"\n",
        "TFX_IMAGE_URI = f\"gcr.io/{GOOGLE_CLOUD_PROJECT}/{DATASET_DISPLAY_NAME}:{VERSION}\"\n",
        "print(f\"URI of the custom image: {TFX_IMAGE_URI}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "URI of the custom image: gcr.io/fast-ai-exploration/flowers:tfx-1-0-0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3YcE0xUgOh8",
        "outputId": "dde308ef-0a85-4064-c425-fd61e067dc92"
      },
      "source": [
        "%%writefile Dockerfile\n",
        "\n",
        "FROM gcr.io/tfx-oss-public/tfx:1.0.0\n",
        "RUN mkdir -p custom_components\n",
        "COPY custom_components/* ./custom_components/\n",
        "RUN pip install --upgrade google-cloud-aiplatform google-cloud-storage firebase-admin"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting Dockerfile\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEydNZrHg6_J"
      },
      "source": [
        "!gcloud builds submit --tag $TFX_IMAGE_URI . --timeout=15m --machine-type=e2-highcpu-8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEbNM9CeERX2"
      },
      "source": [
        "# Specify training worker configurations. To minimize costs we can even specify two\n",
        "# different configurations: a beefier machine for the Endpoint model and slightly less\n",
        "# powerful machine for the mobile model.\n",
        "TRAINING_JOB_SPEC = {\n",
        "    'project': GOOGLE_CLOUD_PROJECT,\n",
        "    'worker_pool_specs': [{\n",
        "        'machine_spec': {\n",
        "            'machine_type': 'n1-standard-4',\n",
        "            'accelerator_type': 'NVIDIA_TESLA_K80',\n",
        "            'accelerator_count': 1\n",
        "        },\n",
        "        'replica_count': 1,\n",
        "        'container_spec': {\n",
        "            'image_uri': 'gcr.io/tfx-oss-public/tfx:{}'.format(tfx.__version__),\n",
        "        },\n",
        "    }],\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln1cvbcfphA9"
      },
      "source": [
        "from custom_components.vertex_uploader import VertexUploader\n",
        "from custom_components.vertex_deployer import VertexDeployer\n",
        "\n",
        "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
        "                     densenet_module_file: str, mobilenet_module_file: str,\n",
        "                     serving_model_dir: str, project_id: str,\n",
        "                     region: str) -> tfx.dsl.Pipeline:\n",
        "    \"\"\"Creates a three component flowers pipeline with TFX.\"\"\"\n",
        "    # Brings data into the pipeline.\n",
        "    # input_base: gs://flowers-public/tfrecords-jpeg-224x224\n",
        "    example_gen = tfx.components.ImportExampleGen(input_base=data_root)\n",
        "\n",
        "    # Uses user-provided Python function that trains a model.\n",
        "    densenet_trainer = tfx.extensions.google_cloud_ai_platform.Trainer(\n",
        "        module_file=densenet_module_file,\n",
        "        examples=example_gen.outputs['examples'],\n",
        "        train_args=tfx.proto.TrainArgs(num_steps=52),\n",
        "        eval_args=tfx.proto.EvalArgs(num_steps=5),\n",
        "        custom_config={\n",
        "            tfx.extensions.google_cloud_ai_platform.ENABLE_UCAIP_KEY:\n",
        "                True,\n",
        "            tfx.extensions.google_cloud_ai_platform.UCAIP_REGION_KEY:\n",
        "                region,\n",
        "            tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY:\n",
        "                TRAINING_JOB_SPEC,\n",
        "            'use_gpu':\n",
        "                True,\n",
        "        }\n",
        "    ).with_id(\"densenet_trainer\")\n",
        "\n",
        "    # Pushes the model to a filesystem destination.\n",
        "    pushed_model_location = os.path.join(serving_model_dir, \"densenet\")\n",
        "    densnet_pusher = tfx.components.Pusher(\n",
        "        model=densenet_trainer.outputs['model'],\n",
        "        push_destination=tfx.proto.PushDestination(\n",
        "            filesystem=tfx.proto.PushDestination.Filesystem(\n",
        "                base_directory=pushed_model_location))).with_id(\"densnet_pusher\")\n",
        "    \n",
        "    # Vertex AI upload.\n",
        "    model_display_name = \"densenet_flowers\"\n",
        "    uploader = VertexUploader(\n",
        "        project=project_id,\n",
        "        region=region,\n",
        "        model_display_name=model_display_name,\n",
        "        pushed_model_location=pushed_model_location,\n",
        "        serving_image_uri=TFX_IMAGE_URI # Using the image we built.\n",
        "    ).with_id(\"vertex_uploader\")\n",
        "    uploader.add_upstream_node(densnet_pusher)\n",
        "\n",
        "    # Create an endpoint.\n",
        "    deployer = VertexDeployer(\n",
        "        project=project_id,\n",
        "        region=region,\n",
        "        model_display_name=model_display_name,\n",
        "        deployed_model_display_name=model_display_name + \"_\" + TIMESTAMP\n",
        "    ).with_id(\"vertex_deployer\")\n",
        "    deployer.add_upstream_node(uploader)\n",
        "\n",
        "    # Same for the MobileNet-based model. But it will be later pushed\n",
        "    # to Firebase since it offers better features for TFLite. For now, we'll\n",
        "    # be pushing the model to a GCS location.\n",
        "    mobilenet_trainer = tfx.extensions.google_cloud_ai_platform.Trainer(\n",
        "        module_file=mobilenet_module_file,\n",
        "        examples=example_gen.outputs['examples'],\n",
        "        train_args=tfx.proto.TrainArgs(num_steps=52),\n",
        "        eval_args=tfx.proto.EvalArgs(num_steps=5),\n",
        "        custom_config={\n",
        "            tfx.extensions.google_cloud_ai_platform.ENABLE_UCAIP_KEY:\n",
        "                True,\n",
        "            tfx.extensions.google_cloud_ai_platform.UCAIP_REGION_KEY:\n",
        "                region,\n",
        "            tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY:\n",
        "                TRAINING_JOB_SPEC,\n",
        "            'use_gpu':\n",
        "                True,\n",
        "        }\n",
        "    ).with_id(\"mobilenet_trainer\")\n",
        "\n",
        "    pushed_location_mobilenet = os.path.join(serving_model_dir, \"mobilenet\")\n",
        "    mobilenet_pusher = tfx.components.Pusher(\n",
        "        model=mobilenet_trainer.outputs['model'],\n",
        "        push_destination=tfx.proto.PushDestination(\n",
        "            filesystem=tfx.proto.PushDestination.Filesystem(\n",
        "                base_directory=pushed_location_mobilenet))).with_id(\"mobilenet_pusher\")\n",
        "\n",
        "    # Following components will be included in the pipeline.\n",
        "    components = [\n",
        "        example_gen,\n",
        "        densenet_trainer, densnet_pusher, uploader, deployer,\n",
        "        mobilenet_trainer, mobilenet_pusher\n",
        "    ]\n",
        "\n",
        "    return tfx.dsl.Pipeline(\n",
        "        pipeline_name=pipeline_name,\n",
        "        pipeline_root=pipeline_root,\n",
        "        components=components)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFdlslfOX54z"
      },
      "source": [
        "## Compile the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AY5Z2tbsbwE"
      },
      "source": [
        "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n",
        "\n",
        "# Important: We need to pass the custom Docker image URI to the\n",
        "# `KubeflowV2DagRunnerConfig` to take effect.\n",
        "runner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n",
        "    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(default_image=TFX_IMAGE_URI),\n",
        "    output_filename=PIPELINE_DEFINITION_FILE)\n",
        "\n",
        "_ = runner.run(\n",
        "    _create_pipeline(\n",
        "        pipeline_name=PIPELINE_NAME,\n",
        "        pipeline_root=PIPELINE_ROOT,\n",
        "        data_root=DATA_ROOT,\n",
        "        densenet_module_file=os.path.join(MODULE_ROOT, _trainer_densenet_module_file),\n",
        "        mobilenet_module_file=os.path.join(MODULE_ROOT, _trainer_mobilenet_module_file),\n",
        "        serving_model_dir=SERVING_MODEL_DIR,\n",
        "        project_id=GOOGLE_CLOUD_PROJECT,\n",
        "        region=GOOGLE_CLOUD_REGION\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocHBJaR_X7x2"
      },
      "source": [
        "## Submit the pipeline for execution to Vertex AI\n",
        "\n",
        "Generally, it's a good idea to first do a local run of the end-to-end pipeline before submitting it an online orchestrator. We can use `tfx.orchestration.LocalDagRunner()` for that but for the purposes of this notebook we won't be doing that. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "3elrtDOus83z",
        "outputId": "59e64640-8750-4544-ab61-dd3926f599fa"
      },
      "source": [
        "from kfp.v2.google import client\n",
        "\n",
        "pipelines_client = client.AIPlatformClient(\n",
        "    project_id=GOOGLE_CLOUD_PROJECT,\n",
        "    region=GOOGLE_CLOUD_REGION,\n",
        ")\n",
        "\n",
        "_ = pipelines_client.create_run_from_job_spec(PIPELINE_DEFINITION_FILE, enable_caching=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:google.auth._default:No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/two-way-vertex-pipelines-20210802074350?project=fast-ai-exploration\" target=\"_blank\" >here</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YX3pFWlhN_jR"
      },
      "source": [
        "The pipeline should come out as the following:\n",
        "\n",
        "![](https://i.ibb.co/W0PH2cp/image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHUZObAiX-Nb"
      },
      "source": [
        "## Listing all the pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "7ScAn0pxVLv2",
        "outputId": "c57c5994-e2a9-4775-f2bc-f32ebd9ecaeb"
      },
      "source": [
        "vertex_ai.init(project=GOOGLE_CLOUD_PROJECT, \n",
        "               location=GOOGLE_CLOUD_REGION, \n",
        "               staging_bucket=\"gs://\" + GCS_BUCKET_NAME)\n",
        "\n",
        "pipeline_df = vertex_ai.get_pipeline_df(PIPELINE_NAME)\n",
        "pipeline_df = pipeline_df[pipeline_df.pipeline_name == PIPELINE_NAME]\n",
        "pipeline_df.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pipeline_name</th>\n",
              "      <th>run_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>two-way-vertex-pipelines</td>\n",
              "      <td>two-way-vertex-pipelines-20210802074350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>two-way-vertex-pipelines</td>\n",
              "      <td>two-way-vertex-pipelines-20210802052731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>two-way-vertex-pipelines</td>\n",
              "      <td>two-way-vertex-pipelines-20210802044313</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              pipeline_name                                 run_name\n",
              "0  two-way-vertex-pipelines  two-way-vertex-pipelines-20210802074350\n",
              "1  two-way-vertex-pipelines  two-way-vertex-pipelines-20210802052731\n",
              "2  two-way-vertex-pipelines  two-way-vertex-pipelines-20210802044313"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-757SqrYhf3"
      },
      "source": [
        "## Making predictions with the Endpoint\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cu0enAd6AYb"
      },
      "source": [
        "# endpoint = vertex_ai.Endpoint('projects/29880397572/locations/us-central1/endpoints/3702996832675168256')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgwJ0DkxmZd_"
      },
      "source": [
        "vertex_ai.init(project=GOOGLE_CLOUD_PROJECT, \n",
        "               location=GOOGLE_CLOUD_REGION, \n",
        "               staging_bucket=\"gs://\" + GCS_BUCKET_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "1fQjT3GzO2lJ",
        "outputId": "aea0cb6a-5099-4615-c4d5-d18eb35f930e"
      },
      "source": [
        "model_display_name = \"densenet_flowers\"\n",
        "deployed_model_display_name = model_display_name + \"_\" + TIMESTAMP\n",
        "\n",
        "endpoints = vertex_ai.Endpoint.list(\n",
        "    filter=f'display_name={deployed_model_display_name}', \n",
        "    order_by=\"update_time\"\n",
        ")\n",
        "\n",
        "if len(endpoints) > 0:\n",
        "    print(f\"Endpoint {deployed_model_display_name} already exists.\")\n",
        "    endpoint = endpoints[-1]\n",
        "else:\n",
        "    endpoint = vertex_ai.Endpoint.create(deployed_model_display_name)\n",
        "\n",
        "model = vertex_ai.Model.list(\n",
        "    filter=f'display_name={model_display_name}',\n",
        "    order_by=\"update_time\"\n",
        ")[-1]\n",
        "\n",
        "endpoint = vertex_ai.Endpoint.list(\n",
        "    filter=f'display_name={deployed_model_display_name}',\n",
        "    order_by=\"update_time\"\n",
        ")[-1]\n",
        "\n",
        "deployed_model = endpoint.deploy(\n",
        "    model=model,\n",
        "    # Syntax from here: https://git.io/JBQDP\n",
        "    traffic_split={\"0\": 100},\n",
        "    machine_type=\"n1-standard-4\",\n",
        "    min_replica_count=1,\n",
        "    max_replica_count=1,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Endpoint densenet_flowers_20210802073647 already exists.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FailedPrecondition",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-24d7585308ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mmachine_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"n1-standard-4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mmin_replica_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mmax_replica_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, model, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, sync)\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0mexplanation_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplanation_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m             \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m             \u001b[0msync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    643\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy\u001b[0;34m(self, model, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, sync)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[0mexplanation_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplanation_metadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mexplanation_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplanation_parameters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m             \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         )\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy_call\u001b[0;34m(cls, api_client, endpoint_resource_name, model_resource_name, endpoint_resource_traffic_split, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata)\u001b[0m\n\u001b[1;32m    905\u001b[0m         )\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         \u001b[0moperation_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     def undeploy(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# pylint: disable=raising-bad-type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPrecondition\u001b[0m: 400 Model server terminated: model server container terminated: exit_code: 1\nreason: \"Error\"\nstarted_at {\n  seconds: 1627905643\n}\nfinished_at {\n  seconds: 1627905643\n}\n. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=29880397572&resource=aiplatform.googleapis.com%252FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%223702996832675168256%22%0Aresource.labels.location%3D%22us-central1%22."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkOvg9bh6D-y"
      },
      "source": [
        "image_path = tf.keras.utils.get_file(\"image.jpg\", \n",
        "                                            \"https://m.economictimes.com/thumb/msid-71307470,width-1201,height-900,resizemode-4,imgsize-1040796/roses.jpg\")\n",
        "\n",
        "image = tf.io.read_file(image_path)\n",
        "image = tf.image.decode_jpeg(image, 3)\n",
        "image = tf.image.resize(image, (224, 224))\n",
        "image = [image.numpy().tolist()]\n",
        "endpoint.predict(image)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}