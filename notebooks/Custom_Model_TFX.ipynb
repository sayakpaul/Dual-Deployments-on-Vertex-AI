{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTVp-9PGYFIO"
   },
   "source": [
    "In this notebook, we will build two custom models - one for Endpoint deployment and the other one for mobile deployment. We will write a TFX pipeline to run their training and export. The entire pipeline will be orchestrated using [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7gJqmqrsfqh"
   },
   "source": [
    "## References\n",
    "\n",
    "This notebook extensively refers to the following two resources and also reuses parts of the code from there: \n",
    "* [Simple TFX Pipeline for Vertex Pipelines](https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/tfx/gcp/vertex_pipelines_simple.ipynb)\n",
    "* [Vertex AI Training with TFX and Vertex Pipelines](https://www.tensorflow.org/tfx/tutorials/tfx/gcp/vertex_pipelines_vertex_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D04aKMGWXjOu"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "I_niUhp_TY1G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "# Use the latest version of pip.\n",
    "%%capture\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade tfx==0.30.0 kfp==1.6.1\n",
    "!pip install -q --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVmgQ6w1oT_Z"
   },
   "source": [
    "### ***Please restart runtime before continuing.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mstgsNHWoiXk",
    "outputId": "312b2335-9036-470a-d79c-8a4266b1344e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome! This command will take you through the configuration of gcloud.\n",
      "\n",
      "Settings from your current configuration [default] are:\n",
      "component_manager:\n",
      "  disable_update_check: 'True'\n",
      "compute:\n",
      "  gce_metadata_read_timeout_sec: '0'\n",
      "\n",
      "Pick configuration to use:\n",
      " [1] Re-initialize this configuration [default] with new settings \n",
      " [2] Create a new configuration\n",
      "Please enter your numeric choice:  "
     ]
    }
   ],
   "source": [
    "!gcloud init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pl8ewjX3oXRx"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqVWpmywXngD"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wptXF0e-UXsT",
    "outputId": "6694e7ee-deab-40e0-b23e-cf52ed97a9e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.5.0\n",
      "TFX version: 1.0.0\n",
      "KFP version: 1.6.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))\n",
    "import kfp\n",
    "print('KFP version: {}'.format(kfp.__version__))\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFYHeepnXxpZ"
   },
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zPVyBrXrW-vu"
   },
   "outputs": [],
   "source": [
    "GOOGLE_CLOUD_PROJECT = 'gcp-ml-172005'     #@param {type:\"string\"}\n",
    "GOOGLE_CLOUD_REGION = 'us-central1'      #@param {type:\"string\"}\n",
    "GCS_BUCKET_NAME = 'demo-experiments-gde-csp'          #@param {type:\"string\"}\n",
    "\n",
    "if not (GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_REGION and GCS_BUCKET_NAME):\n",
    "    from absl import logging\n",
    "    logging.error('Please set all required parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J65KHrt4X-Fu",
    "outputId": "a958ffa3-3041-4716-e642-68de010fbba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIPELINE_ROOT: gs://demo-experiments-gde-csp/pipeline_root/two-way-vertex-pipelines\n"
     ]
    }
   ],
   "source": [
    "PIPELINE_NAME = 'two-way-vertex-pipelines'\n",
    "\n",
    "# Path to various pipeline artifact.\n",
    "PIPELINE_ROOT = 'gs://{}/pipeline_root/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# Paths for users' Python module.\n",
    "MODULE_ROOT = 'gs://{}/pipeline_module/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "# Paths for input data.\n",
    "DATA_ROOT = 'gs://flowers-public/tfrecords-jpeg-224x224'\n",
    "\n",
    "# This is the path where your model will be pushed for serving.\n",
    "SERVING_MODEL_DIR = 'gs://{}/serving_model/{}'.format(\n",
    "    GCS_BUCKET_NAME, PIPELINE_NAME)\n",
    "\n",
    "print('PIPELINE_ROOT: {}'.format(PIPELINE_ROOT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQVpzyftX0y0"
   },
   "source": [
    "## Create training modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AR5pU65m6nAE"
   },
   "outputs": [],
   "source": [
    "_trainer_densenet_module_file = 'flower_densenet_trainer.py'\n",
    "_trainer_mobilenet_module_file = 'flower_mobilenet_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XqwYrR1GYLvm",
    "outputId": "e66336d9-40a4-454e-a803-1eaf3fc206cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing flower_densenet_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_trainer_densenet_module_file}\n",
    "\n",
    "from typing import List\n",
    "from absl import logging\n",
    "from tensorflow import keras\n",
    "from tfx import v1 as tfx\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "_IMAGE_FEATURES = {\n",
    "    \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string means bytestring\n",
    "    \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\n",
    "    \"one_hot_class\": tf.io.VarLenFeature(tf.float32),\n",
    "}\n",
    "\n",
    "_INPUT_SHAPE = (224, 224, 3)\n",
    "_TRAIN_BATCH_SIZE = 64\n",
    "_EVAL_BATCH_SIZE = 64\n",
    "_EPOCHS = 10\n",
    "\n",
    "\n",
    "def _parse_fn(example):\n",
    "    example = tf.io.parse_single_example(example, _IMAGE_FEATURES)\n",
    "    image = tf.image.decode_jpeg(example[\"image\"], channels=3)\n",
    "    class_label = tf.cast(example[\"class\"], tf.int32)\n",
    "    return image, class_label\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[str], batch_size: int) -> tf.data.Dataset:\n",
    "    \"\"\"Generates features and label for training.\n",
    "\n",
    "    Args:\n",
    "        file_pattern: List of paths or patterns of input tfrecord files.\n",
    "        batch_size: representing the number of consecutive elements of returned\n",
    "            dataset to combine in a single batch.\n",
    "\n",
    "    Returns:\n",
    "        A dataset that contains (features, indices) tuple where features is a\n",
    "            dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Reading data from: {file_pattern}\")\n",
    "    tfrecord_filenames = tf.io.gfile.glob(file_pattern[0] + \".gz\")\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "    dataset = dataset.map(_parse_fn).batch(batch_size)\n",
    "    return dataset.repeat()\n",
    "\n",
    "\n",
    "def _make_keras_model() -> tf.keras.Model:\n",
    "    \"\"\"Creates a DenseNet121-bases model for classifying flowers data.\n",
    "\n",
    "    Returns:\n",
    "    A Keras Model.\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=_INPUT_SHAPE)\n",
    "    base_model = keras.applications.DenseNet121(\n",
    "        include_top=False, input_shape=_INPUT_SHAPE, pooling=\"avg\"\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "    x = keras.applications.densenet.preprocess_input(inputs)\n",
    "    x = base_model(\n",
    "        x, training=False\n",
    "    )  # Ensures BatchNorm runs in inference model in this model\n",
    "    outputs = keras.layers.Dense(5, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    model.summary(print_fn=logging.info)\n",
    "    return model\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "    \"\"\"Train the model based on given args.\n",
    "\n",
    "    Args:\n",
    "        fn_args: Holds args used to train the model as name/value pairs.\n",
    "    \"\"\"\n",
    "    train_dataset = _input_fn(fn_args.train_files, batch_size=_TRAIN_BATCH_SIZE)\n",
    "    eval_dataset = _input_fn(fn_args.eval_files, batch_size=_EVAL_BATCH_SIZE)\n",
    "\n",
    "    model = _make_keras_model()\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        epochs=_EPOCHS,\n",
    "    )\n",
    "    _, acc = model.evaluate(eval_dataset, steps=fn_args.eval_steps)\n",
    "    logging.info(f\"Validation accuracy: {round(acc * 100, 2)}%\")\n",
    "    # The result of the training should be saved in `fn_args.serving_model_dir`\n",
    "    # directory.\n",
    "    model.save(fn_args.serving_model_dir, save_format=\"tf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tIFDOw5cmqd1",
    "outputId": "5bef6ba0-4409-4745-8c03-9086777dafc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing flower_mobilenet_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_trainer_mobilenet_module_file}\n",
    "\n",
    "from typing import List\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "\n",
    "_IMAGE_FEATURES = {\n",
    "    \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string means bytestring\n",
    "    \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\n",
    "    \"one_hot_class\": tf.io.VarLenFeature(tf.float32),\n",
    "}\n",
    "\n",
    "_INPUT_SHAPE = (224, 224, 3)\n",
    "_TRAIN_BATCH_SIZE = 64\n",
    "_EVAL_BATCH_SIZE = 64\n",
    "_EPOCHS = 10\n",
    "\n",
    "\n",
    "def _parse_fn(example):\n",
    "    example = tf.io.parse_single_example(example, _IMAGE_FEATURES)\n",
    "    image = tf.image.decode_jpeg(example[\"image\"], channels=3)\n",
    "    class_label = tf.cast(example[\"class\"], tf.int32)\n",
    "    return image, class_label\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[str], batch_size: int) -> tf.data.Dataset:\n",
    "    \"\"\"Generates features and label for training.\n",
    "\n",
    "    Args:\n",
    "        file_pattern: List of paths or patterns of input tfrecord files.\n",
    "        batch_size: representing the number of consecutive elements of returned\n",
    "            dataset to combine in a single batch.\n",
    "\n",
    "    Returns:\n",
    "        A dataset that contains (features, indices) tuple where features is a\n",
    "            dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Reading data from: {file_pattern}\")\n",
    "    tfrecord_filenames = tf.io.gfile.glob(file_pattern[0] + \".gz\")\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "    dataset = dataset.map(_parse_fn).batch(batch_size)\n",
    "    return dataset.repeat()\n",
    "\n",
    "\n",
    "def _make_keras_model() -> tf.keras.Model:\n",
    "    \"\"\"Creates a MobileNetV3-bases model for classifying flowers data.\n",
    "\n",
    "    Returns:\n",
    "    A Keras Model.\n",
    "    \"\"\"\n",
    "    inputs = keras.Input(shape=_INPUT_SHAPE)\n",
    "    base_model = keras.applications.MobileNetV3Small(\n",
    "        include_top=False, input_shape=_INPUT_SHAPE, pooling=\"avg\"\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "    x = keras.applications.mobilenet_v3.preprocess_input(inputs)\n",
    "    x = base_model(\n",
    "        x, training=False\n",
    "    )  # Ensures BatchNorm runs in inference model in this model\n",
    "    outputs = keras.layers.Dense(5, activation=\"softmax\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    model.summary(print_fn=logging.info)\n",
    "    return model\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "    \"\"\"Train the model based on given args.\n",
    "\n",
    "    Args:\n",
    "        fn_args: Holds args used to train the model as name/value pairs.\n",
    "    \"\"\"\n",
    "    train_dataset = _input_fn(fn_args.train_files, batch_size=_TRAIN_BATCH_SIZE)\n",
    "    eval_dataset = _input_fn(fn_args.eval_files, batch_size=_EVAL_BATCH_SIZE)\n",
    "\n",
    "    model = _make_keras_model()\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        steps_per_epoch=fn_args.train_steps,\n",
    "        validation_data=eval_dataset,\n",
    "        validation_steps=fn_args.eval_steps,\n",
    "        epochs=_EPOCHS,\n",
    "    )\n",
    "    _, acc = model.evaluate(eval_dataset, steps=fn_args.eval_steps)\n",
    "    logging.info(f\"Validation accuracy: {round(acc * 100, 2)}%\")\n",
    "    # The result of the training should be saved in `fn_args.serving_model_dir`\n",
    "    # directory.\n",
    "    model.save(fn_args.serving_model_dir, save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gp7RTxpMoLe9",
    "outputId": "2022d2c0-980c-4313-85bd-bcdd9da72779"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://flower_densenet_trainer.py [Content-Type=text/x-python]...\n",
      "Copying file://flower_mobilenet_trainer.py [Content-Type=text/x-python]...      \n",
      "/ [2 files][  6.4 KiB/  6.4 KiB]                                                \n",
      "Operation completed over 2 objects/6.4 KiB.                                      \n",
      "  3.21 KiB  2021-08-01T11:05:02Z  gs://demo-experiments-gde-csp/pipeline_module/two-way-vertex-pipelines/flower_densenet_trainer.py\n",
      "  3.22 KiB  2021-08-01T11:05:02Z  gs://demo-experiments-gde-csp/pipeline_module/two-way-vertex-pipelines/flower_mobilenet_trainer.py\n",
      "TOTAL: 2 objects, 6581 bytes (6.43 KiB)\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r *.py {MODULE_ROOT}/\n",
    "!gsutil ls -lh {MODULE_ROOT}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qX4M-C6X4LF"
   },
   "source": [
    "## Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MzZdpUjypcsT"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SYaypU11e-cG"
   },
   "outputs": [],
   "source": [
    "_vertex_uploader_module_file = 'vertex_uploader.py'\n",
    "_vertex_deployer_module_file = 'vertex_deployer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pBj_WnLw6oRR",
    "outputId": "678e3b1a-556e-4e7d-dca3-3f377c289582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vertex_uploader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_vertex_uploader_module_file}\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tfx.dsl.component.experimental.decorators import component\n",
    "from tfx.dsl.component.experimental.annotations import Parameter\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from tfx import v1 as tfx\n",
    "from absl import logging\n",
    "\n",
    "@component\n",
    "def VertexUploader(\n",
    "    project: Parameter[str],\n",
    "    region: Parameter[str],\n",
    "    model_display_name: Parameter[str],\n",
    "    pushed_model_location: Parameter[str],\n",
    "    serving_image_uri: Parameter[str],\n",
    ") -> tfx.dsl.components.OutputDict(model_name=str):\n",
    "\n",
    "    vertex_ai.init(project=project, location=region)\n",
    "\n",
    "    pushed_model_dir = os.path.join(\n",
    "        pushed_model_location, tf.io.gfile.listdir(pushed_model_location)[-1]\n",
    "    )\n",
    "\n",
    "    logging.info(f\"Model registry location: {pushed_model_dir}\")\n",
    "\n",
    "    vertex_model = vertex_ai.Model.upload(\n",
    "        display_name=model_display_name,\n",
    "        artifact_uri=pushed_model_dir,\n",
    "        serving_container_image_uri=serving_image_uri,\n",
    "        parameters_schema_uri=None,\n",
    "        instance_schema_uri=None,\n",
    "        explanation_metadata=None,\n",
    "        explanation_parameters=None,\n",
    "    )\n",
    "\n",
    "    # model_uri = vertex_model.gca_resource.name\n",
    "    # logging.info(f\"Model uploaded to: {model_uri}\")\n",
    "    return {\"model_name\": str(vertex_model.display_name)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "awNiJiZ1fZsp",
    "outputId": "85dbade8-ed34-4c3c-a49a-9631f947d5e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vertex_deployer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_vertex_deployer_module_file}\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tfx.dsl.component.experimental.decorators import component\n",
    "from tfx.dsl.component.experimental.annotations import Parameter\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from tfx import v1 as tfx\n",
    "from absl import logging\n",
    "\n",
    "@component\n",
    "def VertexDeployer(\n",
    "    project: Parameter[str],\n",
    "    region: Parameter[str],\n",
    "    model_name: Parameter[str],\n",
    "    machine_type: Parameter[str],\n",
    "    deployed_model_display_name: Parameter[str],\n",
    ") -> tfx.dsl.components.OutputDict(endpoint_name=str):\n",
    "\n",
    "    vertex_ai.init(project=project, location=region)\n",
    "\n",
    "    model = vertex_ai.Model(model_name=model_name)\n",
    "    endpoint = model.deploy(\n",
    "        deployed_model_display_name=deployed_model_display_name,\n",
    "        traffic_split={\"0\": 100},\n",
    "        machine_type=machine_type,\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=1,\n",
    "        sync=True,\n",
    "    )\n",
    "    logging.info(f\"Model deployed to: {endpoint}\")\n",
    "    return {\"endpoint_name\": str(endpoint)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./custom_components\n",
    "!touch ./custom_components/__init__.py\n",
    "!cp -r {_vertex_uploader_module_file} {_vertex_deployer_module_file} custom_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 12K\n",
      "-rw-r--r-- 1 jupyter jupyter    0 Aug  1 12:28 __init__.py\n",
      "drwxr-xr-x 2 jupyter jupyter 4.0K Aug  1 12:33 __pycache__\n",
      "-rw-r--r-- 1 jupyter jupyter  971 Aug  1 12:28 vertex_deployer.py\n",
      "-rw-r--r-- 1 jupyter jupyter 1.3K Aug  1 12:28 vertex_uploader.py\n"
     ]
    }
   ],
   "source": [
    "!ls -lh custom_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h9j867lPfyM8",
    "outputId": "e0c9b2b5-5e5b-450b-8d3b-d5265f4b27c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URI of the custom image: gcr.io/gcp-ml-172005/flowers:tfx-1-0-0\n"
     ]
    }
   ],
   "source": [
    "DATASET_DISPLAY_NAME = \"flowers\"\n",
    "VERSION = \"tfx-1-0-0\"\n",
    "TFX_IMAGE_URI = f\"gcr.io/{GOOGLE_CLOUD_PROJECT}/{DATASET_DISPLAY_NAME}:{VERSION}\"\n",
    "print(f\"URI of the custom image: {TFX_IMAGE_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F3YcE0xUgOh8",
    "outputId": "433df7f4-9d5c-4554-e354-f3923cc41184"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "\n",
    "FROM gcr.io/tfx-oss-public/tfx:1.0.0\n",
    "RUN mkdir -p custom_components\n",
    "COPY custom_components/* ./custom_components/\n",
    "RUN pip install --upgrade google-cloud-aiplatform google-cloud-storage firebase-admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "hEydNZrHg6_J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 19 file(s) totalling 112.0 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://gcp-ml-172005_cloudbuild/source/1627823544.128529-424decf9cac9466aa9f1b9743f06635c.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/gcp-ml-172005/locations/global/builds/dd33b840-f8d7-4931-8d7a-844478a4bbb5].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/dd33b840-f8d7-4931-8d7a-844478a4bbb5?project=874401645461].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"dd33b840-f8d7-4931-8d7a-844478a4bbb5\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://gcp-ml-172005_cloudbuild/source/1627823544.128529-424decf9cac9466aa9f1b9743f06635c.tgz#1627823544576339\n",
      "Copying gs://gcp-ml-172005_cloudbuild/source/1627823544.128529-424decf9cac9466aa9f1b9743f06635c.tgz#1627823544576339...\n",
      "/ [1 files][ 21.5 KiB/ 21.5 KiB]                                                \n",
      "Operation completed over 1 objects/21.5 KiB.                                     \n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  132.1kB\n",
      "Step 1/4 : FROM gcr.io/tfx-oss-public/tfx:1.0.0\n",
      "1.0.0: Pulling from tfx-oss-public/tfx\n",
      "25fa05cd42bd: Pulling fs layer\n",
      "2d6e353a95ec: Pulling fs layer\n",
      "14d7996407de: Pulling fs layer\n",
      "0c9c6fc70f16: Pulling fs layer\n",
      "c3c76be11512: Pulling fs layer\n",
      "ab6e5a9c78ee: Pulling fs layer\n",
      "7bc1690abd59: Pulling fs layer\n",
      "f5b4dd7682bc: Pulling fs layer\n",
      "d6897660f71d: Pulling fs layer\n",
      "174d792fb622: Pulling fs layer\n",
      "a7a7ae3235b8: Pulling fs layer\n",
      "e4790a266767: Pulling fs layer\n",
      "daba7cb3f799: Pulling fs layer\n",
      "55d5ef773782: Pulling fs layer\n",
      "52771a439fa8: Pulling fs layer\n",
      "810d1f8c7f15: Pulling fs layer\n",
      "6e1d6dba0d16: Pulling fs layer\n",
      "b078c2022d55: Pulling fs layer\n",
      "69577f1d702f: Pulling fs layer\n",
      "9137b84f7e19: Pulling fs layer\n",
      "98990c035a71: Pulling fs layer\n",
      "582d3e1b0799: Pulling fs layer\n",
      "123a5c227305: Pulling fs layer\n",
      "e1a94bcd15d3: Pulling fs layer\n",
      "9a991d5f7266: Pulling fs layer\n",
      "45439985d602: Pulling fs layer\n",
      "673e674e91bd: Pulling fs layer\n",
      "ecc3ce1cf47f: Pulling fs layer\n",
      "75df750e0c48: Pulling fs layer\n",
      "39347a0726ae: Pulling fs layer\n",
      "39705430dced: Pulling fs layer\n",
      "195d3ff7189f: Pulling fs layer\n",
      "6657d01ec620: Pulling fs layer\n",
      "273173b54e2b: Pulling fs layer\n",
      "582d3e1b0799: Waiting\n",
      "123a5c227305: Waiting\n",
      "e1a94bcd15d3: Waiting\n",
      "9a991d5f7266: Waiting\n",
      "45439985d602: Waiting\n",
      "f5b4dd7682bc: Waiting\n",
      "673e674e91bd: Waiting\n",
      "ecc3ce1cf47f: Waiting\n",
      "d6897660f71d: Waiting\n",
      "75df750e0c48: Waiting\n",
      "174d792fb622: Waiting\n",
      "a7a7ae3235b8: Waiting\n",
      "39347a0726ae: Waiting\n",
      "e4790a266767: Waiting\n",
      "39705430dced: Waiting\n",
      "daba7cb3f799: Waiting\n",
      "195d3ff7189f: Waiting\n",
      "6657d01ec620: Waiting\n",
      "55d5ef773782: Waiting\n",
      "273173b54e2b: Waiting\n",
      "52771a439fa8: Waiting\n",
      "9137b84f7e19: Waiting\n",
      "98990c035a71: Waiting\n",
      "810d1f8c7f15: Waiting\n",
      "6e1d6dba0d16: Waiting\n",
      "b078c2022d55: Waiting\n",
      "69577f1d702f: Waiting\n",
      "0c9c6fc70f16: Waiting\n",
      "7bc1690abd59: Waiting\n",
      "2d6e353a95ec: Verifying Checksum\n",
      "2d6e353a95ec: Download complete\n",
      "0c9c6fc70f16: Verifying Checksum\n",
      "0c9c6fc70f16: Download complete\n",
      "14d7996407de: Verifying Checksum\n",
      "14d7996407de: Download complete\n",
      "25fa05cd42bd: Verifying Checksum\n",
      "25fa05cd42bd: Download complete\n",
      "c3c76be11512: Verifying Checksum\n",
      "c3c76be11512: Download complete\n",
      "7bc1690abd59: Verifying Checksum\n",
      "7bc1690abd59: Download complete\n",
      "d6897660f71d: Verifying Checksum\n",
      "d6897660f71d: Download complete\n",
      "25fa05cd42bd: Pull complete\n",
      "2d6e353a95ec: Pull complete\n",
      "14d7996407de: Pull complete\n",
      "0c9c6fc70f16: Pull complete\n",
      "c3c76be11512: Pull complete\n",
      "f5b4dd7682bc: Verifying Checksum\n",
      "f5b4dd7682bc: Download complete\n",
      "a7a7ae3235b8: Verifying Checksum\n",
      "a7a7ae3235b8: Download complete\n",
      "174d792fb622: Verifying Checksum\n",
      "174d792fb622: Download complete\n",
      "ab6e5a9c78ee: Verifying Checksum\n",
      "ab6e5a9c78ee: Download complete\n",
      "55d5ef773782: Verifying Checksum\n",
      "55d5ef773782: Download complete\n",
      "52771a439fa8: Download complete\n",
      "daba7cb3f799: Verifying Checksum\n",
      "daba7cb3f799: Download complete\n",
      "6e1d6dba0d16: Verifying Checksum\n",
      "6e1d6dba0d16: Download complete\n",
      "b078c2022d55: Verifying Checksum\n",
      "b078c2022d55: Download complete\n",
      "69577f1d702f: Verifying Checksum\n",
      "69577f1d702f: Download complete\n",
      "9137b84f7e19: Verifying Checksum\n",
      "9137b84f7e19: Download complete\n",
      "98990c035a71: Verifying Checksum\n",
      "98990c035a71: Download complete\n",
      "582d3e1b0799: Verifying Checksum\n",
      "582d3e1b0799: Download complete\n",
      "123a5c227305: Verifying Checksum\n",
      "123a5c227305: Download complete\n",
      "e1a94bcd15d3: Verifying Checksum\n",
      "e1a94bcd15d3: Download complete\n",
      "e4790a266767: Download complete\n",
      "810d1f8c7f15: Verifying Checksum\n",
      "810d1f8c7f15: Download complete\n",
      "9a991d5f7266: Verifying Checksum\n",
      "9a991d5f7266: Download complete\n",
      "673e674e91bd: Verifying Checksum\n",
      "673e674e91bd: Download complete\n",
      "75df750e0c48: Verifying Checksum\n",
      "75df750e0c48: Download complete\n",
      "39347a0726ae: Verifying Checksum\n",
      "39347a0726ae: Download complete\n",
      "39705430dced: Verifying Checksum\n",
      "39705430dced: Download complete\n",
      "195d3ff7189f: Verifying Checksum\n",
      "195d3ff7189f: Download complete\n",
      "6657d01ec620: Verifying Checksum\n",
      "6657d01ec620: Download complete\n",
      "273173b54e2b: Verifying Checksum\n",
      "273173b54e2b: Download complete\n",
      "ecc3ce1cf47f: Download complete\n",
      "45439985d602: Verifying Checksum\n",
      "45439985d602: Download complete\n",
      "ab6e5a9c78ee: Pull complete\n",
      "7bc1690abd59: Pull complete\n",
      "f5b4dd7682bc: Pull complete\n",
      "d6897660f71d: Pull complete\n",
      "174d792fb622: Pull complete\n",
      "a7a7ae3235b8: Pull complete\n",
      "e4790a266767: Pull complete\n",
      "daba7cb3f799: Pull complete\n",
      "55d5ef773782: Pull complete\n",
      "52771a439fa8: Pull complete\n",
      "810d1f8c7f15: Pull complete\n",
      "6e1d6dba0d16: Pull complete\n",
      "b078c2022d55: Pull complete\n",
      "69577f1d702f: Pull complete\n",
      "9137b84f7e19: Pull complete\n",
      "98990c035a71: Pull complete\n",
      "582d3e1b0799: Pull complete\n",
      "123a5c227305: Pull complete\n",
      "e1a94bcd15d3: Pull complete\n",
      "9a991d5f7266: Pull complete\n",
      "45439985d602: Pull complete\n",
      "673e674e91bd: Pull complete\n",
      "ecc3ce1cf47f: Pull complete\n",
      "75df750e0c48: Pull complete\n",
      "39347a0726ae: Pull complete\n",
      "39705430dced: Pull complete\n",
      "195d3ff7189f: Pull complete\n",
      "6657d01ec620: Pull complete\n",
      "273173b54e2b: Pull complete\n",
      "Digest: sha256:0f664a48a3fd9de63e80db49dee75c90fa7cea71cb3d62c459cdca1a6b4092b6\n",
      "Status: Downloaded newer image for gcr.io/tfx-oss-public/tfx:1.0.0\n",
      " ---> ec18173ab098\n",
      "Step 2/4 : RUN mkdir -p custom_components\n",
      " ---> Running in 9fa46c167926\n",
      "Removing intermediate container 9fa46c167926\n",
      " ---> 95bd91a70fb3\n",
      "Step 3/4 : COPY custom_components/* ./custom_components/\n",
      " ---> 3990a8b82b62\n",
      "Step 4/4 : RUN pip install --upgrade google-cloud-aiplatform google-cloud-storage firebase-admin\n",
      " ---> Running in 3b8d68033ca9\n",
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.7/site-packages (0.7.1)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.1.1-py2.py3-none-any.whl (1.2 MB)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.7/site-packages (1.40.0)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-1.41.1-py2.py3-none-any.whl (105 kB)\n",
      "Collecting firebase-admin\n",
      "  Downloading firebase_admin-5.0.1-py3-none-any.whl (113 kB)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.19.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.22.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.30.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.20.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (20.9)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.3.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.7.1)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.24.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (1.32.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.25.1)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (2021.1)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (3.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.53.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (49.6.0.post20210108)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-aiplatform) (1.34.1)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.24.0->google-cloud-storage) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.24.0->google-cloud-storage) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.24.0->google-cloud-storage) (0.2.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage) (1.14.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<3.0dev,>=1.3.0->google-cloud-storage) (2.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14.3->google-cloud-aiplatform) (2.4.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.24.0->google-cloud-storage) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.6)\n",
      "Collecting cachecontrol>=0.12.6\n",
      "  Downloading CacheControl-0.12.6-py2.py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: google-api-python-client>=1.7.8 in /opt/conda/lib/python3.7/site-packages (from firebase-admin) (1.12.8)\n",
      "Requirement already satisfied: google-cloud-firestore>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from firebase-admin) (2.1.3)\n",
      "Collecting msgpack>=0.5.2\n",
      "  Downloading msgpack-1.0.2-cp37-cp37m-manylinux1_x86_64.whl (273 kB)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.7.8->firebase-admin) (0.1.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.7.8->firebase-admin) (3.0.1)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client>=1.7.8->firebase-admin) (0.19.1)\n",
      "Installing collected packages: msgpack, google-cloud-storage, cachecontrol, google-cloud-aiplatform, firebase-admin\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 1.40.0\n",
      "    Uninstalling google-cloud-storage-1.40.0:\n",
      "      Successfully uninstalled google-cloud-storage-1.40.0\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 0.7.1\n",
      "    Uninstalling google-cloud-aiplatform-0.7.1:\n",
      "      Successfully uninstalled google-cloud-aiplatform-0.7.1\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx 1.0.0 requires google-cloud-aiplatform<0.8,>=0.5.0, but you have google-cloud-aiplatform 1.1.1 which is incompatible.\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Successfully installed cachecontrol-0.12.6 firebase-admin-5.0.1 google-cloud-aiplatform-1.1.1 google-cloud-storage-1.41.1 msgpack-1.0.2\n",
      "Removing intermediate container 3b8d68033ca9\n",
      " ---> 80b1e659912c\n",
      "Successfully built 80b1e659912c\n",
      "Successfully tagged gcr.io/gcp-ml-172005/flowers:tfx-1-0-0\n",
      "PUSH\n",
      "Pushing gcr.io/gcp-ml-172005/flowers:tfx-1-0-0\n",
      "The push refers to repository [gcr.io/gcp-ml-172005/flowers]\n",
      "4ea72c4f0fbb: Preparing\n",
      "77d2c1e35ce2: Preparing\n",
      "251ce9017f2f: Preparing\n",
      "6d858c512241: Preparing\n",
      "be71d74629c8: Preparing\n",
      "88a698d8d44c: Preparing\n",
      "1cefd89685e2: Preparing\n",
      "50f299fadb57: Preparing\n",
      "c5b927425d39: Preparing\n",
      "69534396463a: Preparing\n",
      "ce435ef6c961: Preparing\n",
      "19a768dfc249: Preparing\n",
      "c0984cbe5fb0: Preparing\n",
      "1ffa79ef958a: Preparing\n",
      "dd3641ae1300: Preparing\n",
      "048849384f76: Preparing\n",
      "60457308415e: Preparing\n",
      "7b7e0814fb39: Preparing\n",
      "8824bcf53be5: Preparing\n",
      "0282a5626fc9: Preparing\n",
      "38fd6253d458: Preparing\n",
      "dc46da60daaa: Preparing\n",
      "7e8afa8049a3: Preparing\n",
      "cea6f389c694: Preparing\n",
      "d50c89cbf864: Preparing\n",
      "4bdcf981f48e: Preparing\n",
      "965a515811e3: Preparing\n",
      "b420a468ca49: Preparing\n",
      "608c205798d1: Preparing\n",
      "0760cd6d4269: Preparing\n",
      "fb4755c89c2a: Preparing\n",
      "22cfb9034da6: Preparing\n",
      "8bec4fbfce85: Preparing\n",
      "3b129ca3db46: Preparing\n",
      "64cb1a1930ab: Preparing\n",
      "600ef5a43f1f: Preparing\n",
      "8f8f0266f834: Preparing\n",
      "dc46da60daaa: Waiting\n",
      "88a698d8d44c: Waiting\n",
      "1cefd89685e2: Waiting\n",
      "50f299fadb57: Waiting\n",
      "7e8afa8049a3: Waiting\n",
      "cea6f389c694: Waiting\n",
      "d50c89cbf864: Waiting\n",
      "c5b927425d39: Waiting\n",
      "4bdcf981f48e: Waiting\n",
      "69534396463a: Waiting\n",
      "ce435ef6c961: Waiting\n",
      "965a515811e3: Waiting\n",
      "b420a468ca49: Waiting\n",
      "19a768dfc249: Waiting\n",
      "c0984cbe5fb0: Waiting\n",
      "608c205798d1: Waiting\n",
      "0760cd6d4269: Waiting\n",
      "1ffa79ef958a: Waiting\n",
      "dd3641ae1300: Waiting\n",
      "fb4755c89c2a: Waiting\n",
      "048849384f76: Waiting\n",
      "22cfb9034da6: Waiting\n",
      "60457308415e: Waiting\n",
      "8bec4fbfce85: Waiting\n",
      "7b7e0814fb39: Waiting\n",
      "8824bcf53be5: Waiting\n",
      "0282a5626fc9: Waiting\n",
      "38fd6253d458: Waiting\n",
      "3b129ca3db46: Waiting\n",
      "64cb1a1930ab: Waiting\n",
      "600ef5a43f1f: Waiting\n",
      "be71d74629c8: Layer already exists\n",
      "6d858c512241: Layer already exists\n",
      "88a698d8d44c: Layer already exists\n",
      "1cefd89685e2: Layer already exists\n",
      "50f299fadb57: Layer already exists\n",
      "c5b927425d39: Layer already exists\n",
      "69534396463a: Layer already exists\n",
      "ce435ef6c961: Layer already exists\n",
      "19a768dfc249: Layer already exists\n",
      "c0984cbe5fb0: Layer already exists\n",
      "1ffa79ef958a: Layer already exists\n",
      "dd3641ae1300: Layer already exists\n",
      "048849384f76: Layer already exists\n",
      "60457308415e: Layer already exists\n",
      "7b7e0814fb39: Layer already exists\n",
      "8824bcf53be5: Layer already exists\n",
      "0282a5626fc9: Layer already exists\n",
      "38fd6253d458: Layer already exists\n",
      "dc46da60daaa: Layer already exists\n",
      "7e8afa8049a3: Layer already exists\n",
      "cea6f389c694: Layer already exists\n",
      "d50c89cbf864: Layer already exists\n",
      "4bdcf981f48e: Layer already exists\n",
      "965a515811e3: Layer already exists\n",
      "b420a468ca49: Layer already exists\n",
      "608c205798d1: Layer already exists\n",
      "0760cd6d4269: Layer already exists\n",
      "fb4755c89c2a: Layer already exists\n",
      "22cfb9034da6: Layer already exists\n",
      "8bec4fbfce85: Layer already exists\n",
      "3b129ca3db46: Layer already exists\n",
      "64cb1a1930ab: Layer already exists\n",
      "600ef5a43f1f: Layer already exists\n",
      "8f8f0266f834: Layer already exists\n",
      "251ce9017f2f: Pushed\n",
      "77d2c1e35ce2: Pushed\n",
      "4ea72c4f0fbb: Pushed\n",
      "tfx-1-0-0: digest: sha256:c65cad495d9a488bc7f1813d8dbd1a43c14b1921a9748694218ead95517ffab9 size: 8098\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                       IMAGES                                  STATUS\n",
      "dd33b840-f8d7-4931-8d7a-844478a4bbb5  2021-08-01T13:12:24+00:00  4M12S     gs://gcp-ml-172005_cloudbuild/source/1627823544.128529-424decf9cac9466aa9f1b9743f06635c.tgz  gcr.io/gcp-ml-172005/flowers:tfx-1-0-0  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --tag $TFX_IMAGE_URI . --timeout=15m --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "sEbNM9CeERX2"
   },
   "outputs": [],
   "source": [
    "# Specify training worker configurations. To minimize costs we can even specify two\n",
    "# different configurations: a beefier machine for the Endpoint model and slightly less\n",
    "# powerful machine for the mobile model.\n",
    "TRAINING_JOB_SPEC = {\n",
    "    'project': GOOGLE_CLOUD_PROJECT,\n",
    "    'worker_pool_specs': [{\n",
    "        'machine_spec': {\n",
    "            'machine_type': 'n1-standard-4',\n",
    "            'accelerator_type': 'NVIDIA_TESLA_K80',\n",
    "            'accelerator_count': 1\n",
    "        },\n",
    "        'replica_count': 1,\n",
    "        'container_spec': {\n",
    "            'image_uri': 'gcr.io/tfx-oss-public/tfx:{}'.format(tfx.__version__),\n",
    "        },\n",
    "    }],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "ln1cvbcfphA9"
   },
   "outputs": [],
   "source": [
    "from custom_components.vertex_uploader import VertexUploader\n",
    "from custom_components.vertex_deployer import VertexDeployer\n",
    "\n",
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     densenet_module_file: str, mobilenet_module_file: str,\n",
    "                     serving_model_dir: str, project_id: str,\n",
    "                     region: str) -> tfx.dsl.Pipeline:\n",
    "    \"\"\"Creates a three component flowers pipeline with TFX.\"\"\"\n",
    "    # Brings data into the pipeline.\n",
    "    # input_base: gs://flowers-public/tfrecords-jpeg-224x224\n",
    "    example_gen = tfx.components.ImportExampleGen(input_base=data_root)\n",
    "\n",
    "    # Uses user-provided Python function that trains a model.\n",
    "    densenet_trainer = tfx.extensions.google_cloud_ai_platform.Trainer(\n",
    "        module_file=densenet_module_file,\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        train_args=tfx.proto.TrainArgs(num_steps=52),\n",
    "        eval_args=tfx.proto.EvalArgs(num_steps=5),\n",
    "        custom_config={\n",
    "            tfx.extensions.google_cloud_ai_platform.ENABLE_UCAIP_KEY:\n",
    "                True,\n",
    "            tfx.extensions.google_cloud_ai_platform.UCAIP_REGION_KEY:\n",
    "                region,\n",
    "            tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY:\n",
    "                TRAINING_JOB_SPEC,\n",
    "            'use_gpu':\n",
    "                True,\n",
    "        }\n",
    "    ).with_id(\"densenet_trainer\")\n",
    "\n",
    "    # Pushes the model to a filesystem destination.\n",
    "    pushed_model_location = os.path.join(serving_model_dir, \"densenet\")\n",
    "    densnet_pusher = tfx.components.Pusher(\n",
    "        model=densenet_trainer.outputs['model'],\n",
    "        push_destination=tfx.proto.PushDestination(\n",
    "            filesystem=tfx.proto.PushDestination.Filesystem(\n",
    "                base_directory=pushed_model_location))).with_id(\"densnet_pusher\")\n",
    "    \n",
    "    # Vertex AI upload.\n",
    "    uploader = VertexUploader(\n",
    "        project=project_id,\n",
    "        region=region,\n",
    "        model_display_name=\"densenet_flowers\",\n",
    "        pushed_model_location=pushed_model_location,\n",
    "        serving_image_uri=TFX_IMAGE_URI # Using the image we built.\n",
    "    ).with_id(\"vertex_uploader\")\n",
    "    uploader.add_upstream_node(densnet_pusher)\n",
    "\n",
    "    # Create an endpoint.\n",
    "    deployer = VertexDeployer(\n",
    "        project=project_id,\n",
    "        region=region,\n",
    "        model_name=str(uploader.outputs[\"model_name\"]),\n",
    "        machine_type=\"n1-standard-4\",\n",
    "        deployed_model_display_name=str(uploader.outputs[\"model_name\"]) + \"_\" + TIMESTAMP\n",
    "    ).with_id(\"vertex_deployer\")\n",
    "    deployer.add_upstream_node(uploader)\n",
    "\n",
    "    # Same for the MobileNet-based model. But it will be later pushed\n",
    "    # to Firebase since it offers better features for TFLite. For now, we'll\n",
    "    # be pushing the model to a GCS location.\n",
    "    mobilenet_trainer = tfx.extensions.google_cloud_ai_platform.Trainer(\n",
    "        module_file=mobilenet_module_file,\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        train_args=tfx.proto.TrainArgs(num_steps=52),\n",
    "        eval_args=tfx.proto.EvalArgs(num_steps=5),\n",
    "        custom_config={\n",
    "            tfx.extensions.google_cloud_ai_platform.ENABLE_UCAIP_KEY:\n",
    "                True,\n",
    "            tfx.extensions.google_cloud_ai_platform.UCAIP_REGION_KEY:\n",
    "                region,\n",
    "            tfx.extensions.google_cloud_ai_platform.TRAINING_ARGS_KEY:\n",
    "                TRAINING_JOB_SPEC,\n",
    "            'use_gpu':\n",
    "                True,\n",
    "        }\n",
    "    ).with_id(\"mobilenet_trainer\")\n",
    "\n",
    "    pushed_location_mobilenet = os.path.join(serving_model_dir, \"mobilenet\")\n",
    "    mobilenet_pusher = tfx.components.Pusher(\n",
    "        model=mobilenet_trainer.outputs['model'],\n",
    "        push_destination=tfx.proto.PushDestination(\n",
    "            filesystem=tfx.proto.PushDestination.Filesystem(\n",
    "                base_directory=pushed_location_mobilenet))).with_id(\"mobilenet_pusher\")\n",
    "\n",
    "    # Following components will be included in the pipeline.\n",
    "    components = [\n",
    "        example_gen,\n",
    "        densenet_trainer, densnet_pusher, uploader, deployer,\n",
    "        mobilenet_trainer, mobilenet_pusher\n",
    "    ]\n",
    "\n",
    "    return tfx.dsl.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFdlslfOX54z"
   },
   "source": [
    "## Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "-AY5Z2tbsbwE"
   },
   "outputs": [],
   "source": [
    "PIPELINE_DEFINITION_FILE = PIPELINE_NAME + '_pipeline.json'\n",
    "\n",
    "runner = tfx.orchestration.experimental.KubeflowV2DagRunner(\n",
    "    config=tfx.orchestration.experimental.KubeflowV2DagRunnerConfig(default_image=TFX_IMAGE_URI),\n",
    "    output_filename=PIPELINE_DEFINITION_FILE)\n",
    "\n",
    "_ = runner.run(\n",
    "    _create_pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        data_root=DATA_ROOT,\n",
    "        densenet_module_file=os.path.join(MODULE_ROOT, _trainer_densenet_module_file),\n",
    "        mobilenet_module_file=os.path.join(MODULE_ROOT, _trainer_mobilenet_module_file),\n",
    "        serving_model_dir=SERVING_MODEL_DIR,\n",
    "        project_id=GOOGLE_CLOUD_PROJECT,\n",
    "        region=GOOGLE_CLOUD_REGION\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocHBJaR_X7x2"
   },
   "source": [
    "## Submit the pipeline for execution to Vertex AI\n",
    "\n",
    "Generally, it's a good idea to first do a local run of the end-to-end pipeline before submitting it an online orchestrator. We can use `tfx.orchestration.LocalDagRunner()` for that but for the purposes of this notebook we won't be doing that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "3elrtDOus83z",
    "outputId": "5ebeb689-a058-4f67-f600-4b7ce659842e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/two-way-vertex-pipelines-20210801131902?project=gcp-ml-172005\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from kfp.v2.google import client\n",
    "\n",
    "pipelines_client = client.AIPlatformClient(\n",
    "    project_id=GOOGLE_CLOUD_PROJECT,\n",
    "    region=GOOGLE_CLOUD_REGION,\n",
    ")\n",
    "\n",
    "_ = pipelines_client.create_run_from_job_spec(PIPELINE_DEFINITION_FILE, enable_caching=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YX3pFWlhN_jR"
   },
   "source": [
    "The pipeline should come out as the following:\n",
    "\n",
    "![](https://i.ibb.co/W0PH2cp/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHUZObAiX-Nb"
   },
   "source": [
    "## Listing all the pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "7ScAn0pxVLv2",
    "outputId": "269838f0-de62-4293-b19e-1b3a835bdb05"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_name</th>\n",
       "      <th>run_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>two-way-vertex-pipelines</td>\n",
       "      <td>two-way-vertex-pipelines-20210730155639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>two-way-vertex-pipelines</td>\n",
       "      <td>two-way-vertex-pipelines-20210730131343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>two-way-vertex-pipelines</td>\n",
       "      <td>two-way-vertex-pipelines-20210730125928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              pipeline_name                                 run_name\n",
       "0  two-way-vertex-pipelines  two-way-vertex-pipelines-20210730155639\n",
       "1  two-way-vertex-pipelines  two-way-vertex-pipelines-20210730131343\n",
       "2  two-way-vertex-pipelines  two-way-vertex-pipelines-20210730125928"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertex_ai.init(project=GOOGLE_CLOUD_PROJECT, \n",
    "               location=GOOGLE_CLOUD_REGION, \n",
    "               staging_bucket=\"gs://\" + GCS_BUCKET_NAME)\n",
    "\n",
    "pipeline_df = vertex_ai.get_pipeline_df(PIPELINE_NAME)\n",
    "pipeline_df = pipeline_df[pipeline_df.pipeline_name == PIPELINE_NAME]\n",
    "pipeline_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-757SqrYhf3"
   },
   "source": [
    "## To be added\n",
    "\n",
    "* Model uploader to Vertex AI with a custom component. [This](https://github.com/GoogleCloudPlatform/mlops-with-vertex-ai/blob/main/src/tfx_pipelines/components.py#L74) will be a good reference. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of Custom_Model_TFX.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-4.mnightly-2021-02-02-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-02-02-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
