{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61cd40e1",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de28066",
   "metadata": {},
   "source": [
    "## Install packages\n",
    "- `firebase-admin` will be used to publish TFLite model to firebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d7830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c708f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install {USER_FLAG} google-cloud-aiplatform --upgrade\n",
    "!pip3 install {USER_FLAG} kfp google-cloud-pipeline-components --upgrade\n",
    "!pip3 install {USER_FLAG} firebase-admin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812c875e",
   "metadata": {},
   "source": [
    "## Restart Jupyter Notebook programatically\n",
    "- in order to reflect the package installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92fb4dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs\n",
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f65e9",
   "metadata": {},
   "source": [
    "## Check KFP(KubeFlow Pipeline) version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55234629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.6.6\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c437adf1",
   "metadata": {},
   "source": [
    "## Setup GCP Project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a4f1da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID:  grounded-atrium-320207\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"grounded-atrium-320207\"\n",
    "\n",
    "# Get your Google Cloud project ID from gcloud\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID: \", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950c18ba",
   "metadata": {},
   "source": [
    "## GCP Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e58f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "# The Google Cloud Notebook product has specific requirements\n",
    "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# If on Google Cloud Notebooks, then don't execute this code\n",
    "if not IS_GOOGLE_CLOUD_NOTEBOOK:\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7221dad0",
   "metadata": {},
   "source": [
    "## Setup GCS Bucket name\n",
    "- this bucket is where everything is going to stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d49e084",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"gs://vertexai_dual_example\"\n",
    "REGION      = \"us-central1\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1690ebb",
   "metadata": {},
   "source": [
    "### Create GCS Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3e85ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://vertexai_dual_example/...\n"
     ]
    }
   ],
   "source": [
    "!gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1d553a",
   "metadata": {},
   "source": [
    "## Setup GCS Path for Pipeline\n",
    "- the pipeline runs are going to be stored (i.e. Metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0c61275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games:/home/jupyter/.local/bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gs://vertexai_dual_example/pipeline_root/chansung'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "\n",
    "USER = \"chansung\"\n",
    "PIPELINE_ROOT = \"{}/pipeline_root/{}\".format(BUCKET_NAME, USER)\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6986b40b",
   "metadata": {},
   "source": [
    "## Build Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff09f9",
   "metadata": {},
   "source": [
    "### Install packages\n",
    "- KFP related\n",
    "- Client API for AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9a75acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.google.client import AIPlatformClient\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e21652",
   "metadata": {},
   "source": [
    "### Define pipeline component to publish TFLite model to Firebase\n",
    "0. please follow the steps described from `Before you begin` section in the official [Deploy and manage custom models with Firebase Admin SDK] documentation. \n",
    "1. download credentials for the Firebase project\n",
    "2. download TFLite model file\n",
    "3. initialize firebase admin\n",
    "4. upload and publish TFLite model from local file to Firebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "57076987",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-storage\", \"firebase-admin\", \"tensorflow\"])\n",
    "def push_to_firebase():\n",
    "    import firebase_admin\n",
    "    from firebase_admin import ml\n",
    "    from firebase_admin import storage\n",
    "    from firebase_admin import credentials\n",
    "    \n",
    "    from google.cloud import storage as gcs_storage\n",
    "    gcs_client = gcs_storage.Client()\n",
    "    blobs = gcs_client.get_bucket(\"firebase-ml-bucket-gde-csp\").list_blobs()\n",
    "    for blob in blobs:\n",
    "        if blob.name == \"grounded-atrium-320207-firebase-adminsdk-5n9sn-20dbda9947.json\":\n",
    "            blob.download_to_filename('credential.json')\n",
    "            break;\n",
    "    \n",
    "    target_blob = sorted(gcs_client.get_bucket(\"output-model-gde-csp\").list_blobs(), reverse=True, key=lambda blob: blob.name.split('/')[-2])[0]\n",
    "    target_blob.download_to_filename('model.tflite')\n",
    "            \n",
    "            \n",
    "    firebase_admin.initialize_app(\n",
    "        credentials.Certificate(\"credential.json\"),\n",
    "        options={\n",
    "            \"storageBucket\": \"grounded-atrium-320207.appspot.com\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Load a tflite file and upload it to Cloud Storage\n",
    "    source = ml.TFLiteGCSModelSource.from_tflite_model_file('model.tflite')\n",
    "\n",
    "    # Create the model object\n",
    "    tflite_format = ml.TFLiteFormat(model_source=source)\n",
    "    model = ml.Model(\n",
    "        display_name=\"example_model\",  # This is the name you use from your app to load the model.\n",
    "        tags=[\"examples\"],             # Optional tags for easier management.\n",
    "        model_format=tflite_format)\n",
    "\n",
    "    # Add the model to your Firebase project and publish it\n",
    "    new_model = ml.create_model(model)\n",
    "    ml.publish_model(new_model.model_id)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d14770",
   "metadata": {},
   "source": [
    "### Define pipeline itself\n",
    "1. Create Vertex AI's managed dataset from CSV\n",
    "2. Define configs, one for cloud model, and the other one for mobile model\n",
    "3. Run parallel processing for two different workflow(each workflow is configured appropriate for each target environment)\n",
    "  - AutoML training can be configured differently for each target environment depending on `model_type`\n",
    "4. Deploying the trained model as well as creating an endpoint is done with `ModelDeployOp` for cloud model\n",
    "5. Export the trained mobile model to a GCS bucket\n",
    "  - publish the exported mobile model to Firebase through push_to_firebase component\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "22e3d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"cloud-mobile-dual-deployment\")\n",
    "def pipeline(project: str = PROJECT_ID):\n",
    "    ds_op = gcc_aip.ImageDatasetCreateOp(\n",
    "       project=project,\n",
    "       display_name=\"flowers-dataset\",\n",
    "       gcs_source=\"gs://dataset-meta-gde-csp/flowers_vertex.csv\",\n",
    "       import_schema_uri=aiplatform.schema.dataset.ioformat.image.multi_label_classification,\n",
    "    )\n",
    "\n",
    "    configs = [\n",
    "       {\n",
    "          \"type\": \"CLOUD\",\n",
    "          \"model_type\": \"CLOUD\",\n",
    "          \"display_name\": \"train-cloud-model\",\n",
    "          \"model_display_name\": \"cloud-model\",\n",
    "          \"budget_milli_node_hours\": 8000,\n",
    "       },\n",
    "       {\n",
    "          \"type\": \"MOBILE\",\n",
    "          \"model_type\": \"MOBILE_TF_VERSATILE_1\",\n",
    "          \"display_name\": \"train-mobile-model\",\n",
    "          \"model_display_name\": \"mobile-model\",\n",
    "          \"budget_milli_node_hours\": 1000,\n",
    "       }\n",
    "    ]\n",
    "\n",
    "    with kfp.dsl.ParallelFor(configs) as config:\n",
    "        training_job_run_op = gcc_aip.AutoMLImageTrainingJobRunOp(\n",
    "            project=project,\n",
    "            display_name=config.display_name,\n",
    "            prediction_type=\"classification\",\n",
    "            multi_label=True,\n",
    "            model_type=config.model_type,\n",
    "            base_model=None,\n",
    "            dataset=ds_op.outputs[\"dataset\"],\n",
    "            model_display_name=config.model_display_name,\n",
    "            training_fraction_split=0.6,\n",
    "            validation_fraction_split=0.2,\n",
    "            test_fraction_split=0.2,\n",
    "            budget_milli_node_hours=config.budget_milli_node_hours,\n",
    "        )\n",
    "\n",
    "        with kfp.dsl.Condition(config.type=='CLOUD'):\n",
    "            endpoint_op = gcc_aip.ModelDeployOp(\n",
    "                project=project,\n",
    "                model=training_job_run_op.outputs[\"model\"]\n",
    "            )\n",
    "\n",
    "        with kfp.dsl.Condition(config.type=='MOBILE'):\n",
    "            endpoint_op = gcc_aip.ModelExportOp( \n",
    "                project=project,\n",
    "                model=training_job_run_op.outputs[\"model\"],\n",
    "                # tflite, edgetpu-tflite, tf-saved-model, tf-js, core-ml, custom-trained\n",
    "                export_format_id=\"tflite\",\n",
    "                artifact_destination=\"gs://output-model-gde-csp/flower-models/\"\n",
    "            )\n",
    "\n",
    "            push_to_firebase_task = push_to_firebase()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a52203",
   "metadata": {},
   "source": [
    "### Compile the pipeline\n",
    "- you will get a json file for the pipeline spec after compiling.\n",
    "  - you will only need this json file to run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "6b0cdaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"cloud-mobile-dual-deployment.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec1f64f",
   "metadata": {},
   "source": [
    "## Run the pipeline on Vertex AI Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbb6c14",
   "metadata": {},
   "source": [
    "### Create client instance to AI Platform (which is Vertex AI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "cc9a0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.google.client import AIPlatformClient\n",
    "\n",
    "api_client = AIPlatformClient(project_id=PROJECT_ID, region=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07274930",
   "metadata": {},
   "source": [
    "### Run the pipeline with the pipeline spec (json file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "f485821f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See the Pipeline job <a href=\"https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/cloud-mobile-dual-deployment-20210729064010?project=grounded-atrium-320207\" target=\"_blank\" >here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = api_client.create_run_from_job_spec(\n",
    "    \"cloud-mobile-dual-deployment.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\"project\": PROJECT_ID},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m75"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
